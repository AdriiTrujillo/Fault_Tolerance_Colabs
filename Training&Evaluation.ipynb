{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training&Evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP2674Y4UnhJzQxgjB7MQGI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdriiTrujillo/Fault_Tolerance_Colabs/blob/main/Training%26Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIwoPXkiKzLS"
      },
      "source": [
        "# DEEP FORWARD NEURAL NETWORK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gbj6_Y060Px"
      },
      "source": [
        "# Imports y permisos\r\n",
        "\r\n",
        "%matplotlib inline\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import sklearn\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import re\r\n",
        "import sys\r\n",
        "\r\n",
        "import os\r\n",
        "from pydrive.auth import GoogleAuth\r\n",
        "from pydrive.drive import GoogleDrive\r\n",
        "from google.colab import auth, files\r\n",
        "from oauth2client.client import GoogleCredentials\r\n",
        "\r\n",
        "from keras.constraints import maxnorm\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.models import load_model, Model\r\n",
        "from keras.models import model_from_json\r\n",
        "from keras.layers import Dense, Dropout, Activation, Input\r\n",
        "from keras.callbacks import EarlyStopping,ModelCheckpoint\r\n",
        "from sklearn.model_selection import GridSearchCV\r\n",
        "from sklearn.model_selection import RandomizedSearchCV\r\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\r\n",
        "from sklearn.model_selection import KFold\r\n",
        "from keras import applications\r\n",
        "from keras import optimizers\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn import preprocessing"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAw4p9vAJ3Wa",
        "outputId": "7ddd60a1-2c54-4b90-adbe-c05e905d6079"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9zp0cubK-uP"
      },
      "source": [
        "## RED1 EXTREMOS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnWLPCh9Lbp8"
      },
      "source": [
        "### DATOS ENTRENAMIENTO\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCJkXs-3PYM7"
      },
      "source": [
        "# Obtención de los datasets de entrenamiento \r\n",
        "bubblesort_path = '/content/drive/Shareddrives/NN4FT_DB/Exp_Adrian/Data Frames/TAREA_2/bubblesort-score.csv'\r\n",
        "dijkstra_path = '/content/drive/Shareddrives/NN4FT_DB/Exp_Adrian/Data Frames/TAREA_2/dijstra-score.csv'\r\n",
        "ndes_path = '/content/drive/Shareddrives/NN4FT_DB/Exp_Adrian/Data Frames/TAREA_2/ndes-score.csv'\r\n",
        "\r\n",
        "\r\n",
        "bubblesort_df = pd.read_csv(bubblesort_path, sep=\";\")\r\n",
        "print(\"bubblesort_df: \", bubblesort_df.shape)\r\n",
        "\r\n",
        "dijkstra_df = pd.read_csv(dijkstra_path, sep=\";\")\r\n",
        "print(\"dijkstra_df: \", dijkstra_df.shape)\r\n",
        "\r\n",
        "ndes_df = pd.read_csv(ndes_path, sep=\";\")\r\n",
        "print(\"ndes_df: \", ndes_df.shape)\r\n",
        "\r\n",
        "# Obtención de los datos de entrenamiento\r\n",
        "\r\n",
        "bubblesort_Xdata = bubblesort_df[[\"time\", \"size\"]].values\r\n",
        "bubblesort_Ydata = bubblesort_df[[\"hang\", \"sdc\"]].values\r\n",
        "\r\n",
        "print(\"bubblesort_Xdata: \", bubblesort_Xdata.shape)\r\n",
        "\r\n",
        "dijkstra_Xdata = dijkstra_df[[\"time\", \"size\"]].values\r\n",
        "dijkstra_Ydata = dijkstra_df[[\"hang\", \"sdc\"]].values\r\n",
        "\r\n",
        "print(\"dijkstra_Xdata: \", dijkstra_Xdata.shape)\r\n",
        "\r\n",
        "ndes_Xdata = ndes_df[[\"time\", \"size\"]].values\r\n",
        "ndes_Ydata = ndes_df[[\"hang\", \"sdc\"]].values\r\n",
        "\r\n",
        "print(\"ndes_Xdata: \", ndes_Xdata.shape)\r\n",
        "\r\n",
        "x_data = np.concatenate((bubblesort_Xdata, dijkstra_Xdata, ndes_Xdata), axis=0)\r\n",
        "y_data = np.concatenate((bubblesort_Ydata, dijkstra_Ydata, ndes_Ydata), axis=0)\r\n",
        "\r\n",
        "# Escalado de los datos utilizados \r\n",
        "y_data = y_data/100 # Se deja en tanto por 1 [0,1)\r\n",
        "scaler = preprocessing.StandardScaler() #Values with mean=0 and standard deviation=1\r\n",
        "x_data = scaler.fit_transform(x_data) \r\n",
        "\r\n",
        "print(\"x_data: \", x_data.shape)\r\n",
        "print(\"y_data: \", y_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yf9V0h3KLes2"
      },
      "source": [
        "### DATOS DE TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNmqR8GVNBVa"
      },
      "source": [
        "# Obtención de los datasets de test\r\n",
        "bubblesort_test_path = '/content/drive/Shareddrives/NN4FT_DB/Exp_Adrian/Data Frames/TAREA_2/bubblesort-test.csv'\r\n",
        "dijkstra_test_path = '/content/drive/Shareddrives/NN4FT_DB/Exp_Adrian/Data Frames/TAREA_2/dijstra-test.csv'\r\n",
        "ndes_test_path = '/content/drive/Shareddrives/NN4FT_DB/Exp_Adrian/Data Frames/TAREA_2/ndes-test.csv'\r\n",
        "\r\n",
        "bubblesort_test_df = pd.read_csv(bubblesort_test_path, sep=\";\")\r\n",
        "print(\"bubblesort_test_df: \", bubblesort_test_df.shape)\r\n",
        "\r\n",
        "dijkstra_test_df = pd.read_csv(dijkstra_test_path, sep=\";\")\r\n",
        "print(\"dijkstra_test_df: \", dijkstra_test_df.shape)\r\n",
        "\r\n",
        "ndes_test_df = pd.read_csv(ndes_test_path, sep=\";\")\r\n",
        "print(\"ndes_test_df: \", ndes_test_df.shape)\r\n",
        "\r\n",
        "# Obtención de los datos de test\r\n",
        "\r\n",
        "bubblesort_test_Xdata = bubblesort_test_df[[\"time\", \"size\"]].values\r\n",
        "bubblesort_test_Ydata = bubblesort_test_df[[\"hang\", \"sdc\"]].values\r\n",
        "\r\n",
        "print(\"bubblesort_test_Xdata: \", bubblesort_test_Xdata.shape)\r\n",
        "\r\n",
        "dijkstra_test_Xdata = dijkstra_test_df[[\"time\", \"size\"]].values\r\n",
        "dijkstra_test_Ydata = dijkstra_test_df[[\"hang\", \"sdc\"]].values\r\n",
        "\r\n",
        "print(\"dijkstra_test_Xdata: \", dijkstra_test_Xdata.shape)\r\n",
        "\r\n",
        "ndes_test_Xdata = ndes_test_df[[\"time\", \"size\"]].values\r\n",
        "ndes_test_Ydata = ndes_test_df[[\"hang\", \"sdc\"]].values\r\n",
        "\r\n",
        "print(\"ndes_test_Xdata: \", ndes_test_Xdata.shape)\r\n",
        "\r\n",
        "x_test = np.concatenate((bubblesort_test_Xdata, dijkstra_test_Xdata, ndes_test_Xdata), axis=0)\r\n",
        "y_test = np.concatenate((bubblesort_test_Ydata, dijkstra_test_Ydata, ndes_test_Ydata), axis=0)\r\n",
        "\r\n",
        "# Se realiza un escalado de los datos\r\n",
        "y_test = y_test/100 # datos en tanto por 1 [0,1)\r\n",
        "scaler = preprocessing.StandardScaler() #Values with mean=0 and standard deviation=1\r\n",
        "x_test = scaler.fit_transform(x_test)\r\n",
        "\r\n",
        "print(\"x_test: \", x_test.shape)\r\n",
        "print(\"y_test: \", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P30jWagLuI5"
      },
      "source": [
        "### ENTRENAMIENTO NORMAL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrHhm3DzOmkw"
      },
      "source": [
        "# Declarar el modelo a utilizar antes de correr esta celda\n",
        "model = nn_model2()\n",
        "model.fit(x=x_data, y=y_data, validation_data=(x_test, y_test), batch_size=16, epochs=400, verbose=2)\n",
        "pesos = 'red1_extremos.hdf5'\n",
        "model.save_weights(pesos)\n",
        " \n",
        "# Resultado de la red para ver que tal ha funcionado el entrenamiento\n",
        "scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Precision: \", scores[1]*100, \"%\")\n",
        "print(\"Error medio absoluto: \", scores[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzIo570HPK6B"
      },
      "source": [
        "# Correr esta celda para guardar los pesos en la carpeta del Drive\n",
        "import shutil\n",
        "save_path = '/content/drive/Shareddrives/NN4FT_DB/Exp_Adrian/Redes Entrenadas'\n",
        " \n",
        "ruta = shutil.move(pesos, save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pMuvOxoNCQ0"
      },
      "source": [
        "### ENTRENAMIENTO CUSTOM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRQXDohLNGJY"
      },
      "source": [
        "# Declarar el modelo a utilizar antes de correr esta celda\n",
        "# Con los parametros optimizados\n",
        "\n",
        "model = nn_optModel_T1()                                           \n",
        "model.fit(x=x_data, y=y_data, validation_data=(x_test, y_test), batch_size=16, epochs= 150 , verbose=2)\n",
        "pesos = 'red1_extremos_custom.hdf5'\n",
        "model.save_weights(pesos)\n",
        " \n",
        "# Resultado de la red para ver que tal ha funcionado el entrenamiento\n",
        "scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Precision: \", scores[1]*100, \"%\")\n",
        "print(\"Error medio absoluto: \", scores[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1AI77mbNGJZ"
      },
      "source": [
        "# Correr esta celda para guardar los pesos en la carpeta del Drive\n",
        "import shutil\n",
        "save_path = '/content/drive/Shareddrives/NN4FT_DB/Exp_Adrian/Redes Entrenadas'\n",
        " \n",
        "ruta = shutil.move(pesos, save_path)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m5AP1GcLpVX"
      },
      "source": [
        "### EVALUACIÓN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56SuZUF7PhGp"
      },
      "source": [
        "# Comentar o descomentar dependiendo de que quieras evaluar.\r\n",
        "# nn_path = '/content/drive/Shareddrives/NN4FT_DB/Exp_Adrian/Redes Entrenadas/red1_extremos.hdf5'\r\n",
        "nn_path = '/content/drive/Shareddrives/NN4FT_DB/Exp_Adrian/Redes Entrenadas/TAREA_2/RED1_EXTREMOS/red1_extremos_custom.hdf5'\r\n",
        "\r\n",
        "# Declarar el modelo a utilizar antes de correr esta celda\r\n",
        "model = nn_optModel_1() # CAMBIAR EL MODELO DEPENDIENDO DE RED (nn_model o nn_optModel_1)\r\n",
        "model.load_weights(nn_path)\r\n",
        "\r\n",
        "# Correr la red nueronal\r\n",
        "print('Corriendo la red ... ')\r\n",
        "scores = model.evaluate(x_test, y_test, verbose=0)\r\n",
        "print(\"Precision: \", scores[1]*100, \"%\")\r\n",
        "print(\"Error medio absoluto: \", scores[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlrHtBowup13"
      },
      "source": [
        "# Resultados [SDC, HANG] que obtiene la red para las entradas dadas\r\n",
        "y_predict = model.predict(x_test)\r\n",
        "\r\n",
        "# Error absoluto de las predicciones \r\n",
        "errores = np.abs(y_test-model.predict(x_test))\r\n",
        "\r\n",
        "# Error medio absoluto obtenido en las predicciones \r\n",
        "mean_error = np.abs(y_test-model.predict(x_test)).mean(axis=0)\r\n",
        "\r\n",
        "# Caulcular los resultados para UnAce\r\n",
        "unAce_predict = 1 - (y_predict[:,0] + y_predict[:,1])\r\n",
        "unAce_real = 1 - (y_test[:,0] + y_test[:,1])\r\n",
        "\r\n",
        "unAce_error = np.abs(unAce_real - unAce_predict)\r\n",
        "unAce_MAE = np.abs(unAce_real - unAce_predict).mean(axis=0)\r\n",
        "\r\n",
        "print('Error medio absoluto (FT) :')\r\n",
        "print('SDC -->', mean_error[0])\r\n",
        "print('HANG -->', mean_error[1])\r\n",
        "print('UnAce -->', unAce_MAE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_tn1f6m16X9"
      },
      "source": [
        "# Dibuja todas las graficas de los datos de HANG y SDC\r\n",
        "dibujar_Graficas(y_predict, y_test, errores)\r\n",
        "# Dibuja todas las graficas de los datos de unAce\r\n",
        "graficas_UnAce(y_predict, y_test, errores, unAce_real, unAce_predict, unAce_error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYF6cBvzQLoY"
      },
      "source": [
        "### GUARDADO DE LOS RESULTADOS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4TvLlFcvLql"
      },
      "source": [
        "results_path = '/content/drive/Shareddrives/NN4FT_DB/Exp_Adrian/Resultados/'\r\n",
        "\r\n",
        "csv = 'red1_extremos_custom.csv' # ENTRADA DEL USUARIO\r\n",
        "with open(results_path + csv, \"w\") as f:\r\n",
        "  f.write('SDC_Real;HANG_Real;UnAce_Real;SDC_Predicho;HANG_Predicho;UnAce_Predicho;error_SDC;error_HANG;error_UnAce\\n')\r\n",
        "  f.close()\r\n",
        "\r\n",
        "string = \"\"\r\n",
        "\r\n",
        "for i in range(y_test.shape[0]):\r\n",
        "  string += (str(y_test[i,0]) + \";\" + str(y_test[i,1]) + \";\" + str(unAce_real[i]) + \";\"\r\n",
        "            + str(y_predict[i,0]) + \";\" + str(y_predict[i,1]) + \";\" + str(unAce_predict[i]) + \";\"\r\n",
        "            + str(errores[i,0]) + \";\" + str(errores[i,1]) + \";\" + str(unAce_error[i]) + \"\\n\")\r\n",
        "  \r\n",
        "  \r\n",
        "with open(results_path + csv, \"a\") as f:\r\n",
        "  f.write(string)\r\n",
        "  f.close()\r\n",
        "  "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YZQm5llK_ML"
      },
      "source": [
        "## RED2 EXTREMOS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WcnrAdvLjuR"
      },
      "source": [
        "### DATOS ENTRENAMIENTO\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoipYT7zRQeS"
      },
      "source": [
        "# Obtención de los datasets de entrenamiento \r\n",
        "bubblesort_path = '/content/drive/Shareddrives/NN4FT_DB/Exp_Adrian/Data Frames/TAREA_2/bubblesort-score.csv'\r\n",
        "ndes_path = '/content/drive/Shareddrives/NN4FT_DB/Exp_Adrian/Data Frames/TAREA_2/ndes-score.csv'\r\n",
        "\r\n",
        "bubblesort_df = pd.read_csv(bubblesort_path, sep=\";\")\r\n",
        "print(\"bubblesort_df: \", bubblesort_df.shape)\r\n",
        "\r\n",
        "ndes_df = pd.read_csv(ndes_path, sep=\";\")\r\n",
        "print(\"ndes_df: \", ndes_df.shape)\r\n",
        "\r\n",
        "# Obtención de los datos de entrenamiento\r\n",
        "\r\n",
        "bubblesort_Xdata = bubblesort_df[[\"time\", \"size\"]].values\r\n",
        "bubblesort_Ydata = bubblesort_df[[\"hang\", \"sdc\"]].values\r\n",
        "\r\n",
        "print(\"bubblesort_Xdata: \", bubblesort_Xdata.shape)\r\n",
        "\r\n",
        "ndes_Xdata = ndes_df[[\"time\", \"size\"]].values\r\n",
        "ndes_Ydata = ndes_df[[\"hang\", \"sdc\"]].values\r\n",
        "\r\n",
        "print(\"ndes_Xdata: \", ndes_Xdata.shape)\r\n",
        "\r\n",
        "x_data = np.concatenate((bubblesort_Xdata, ndes_Xdata), axis=0)\r\n",
        "y_data = np.concatenate((bubblesort_Ydata, ndes_Ydata), axis=0)\r\n",
        "\r\n",
        "# Escalado de los datos utilizados \r\n",
        "y_data = y_data/100 # Se deja en tanto por 1 [0,1)\r\n",
        "scaler = preprocessing.StandardScaler() #Values with mean=0 and standard deviation=1\r\n",
        "x_data = scaler.fit_transform(x_data) \r\n",
        "\r\n",
        "print(\"x_data: \", x_data.shape)\r\n",
        "print(\"y_data: \", y_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkF8Df6bLjub"
      },
      "source": [
        "### DATOS DE TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "it0ZLrq0RpWA"
      },
      "source": [
        "# Obtención de los datasets de test\r\n",
        "dijkstra_path = '/content/drive/Shareddrives/NN4FT_DB/Exp_Adrian/Data Frames/TAREA_2/dijkstra_full.csv'\r\n",
        "\r\n",
        "dijkstra_df = pd.read_csv(dijkstra_path, sep=\";\")\r\n",
        "print(\"dijkstra_df: \", dijkstra_df.shape)\r\n",
        "\r\n",
        "# Para obtener el dataset de test se debe indicar un porcentaje del tamaño total (112)\r\n",
        "porcentaje_ind = 75 # Entrada del usuario (Porcentaje)\r\n",
        "num_ind = (len(dijkstra_df)*porcentaje_ind)//100\r\n",
        "print(\"El numero de individuos para la realización de test son: \", num_ind)\r\n",
        "\r\n",
        "dijkstra_test_df = dijkstra_df.sample(num_ind).reset_index(drop=True)\r\n",
        "\r\n",
        "print(\"Tamaño del dataset de Test: \", dijkstra_test_df.shape)\r\n",
        "\r\n",
        "# Obtención de los datos de test\r\n",
        "dijkstra_test_Xdata = dijkstra_test_df[[\"time\", \"size\"]].values\r\n",
        "dijkstra_test_Ydata = dijkstra_test_df[[\"hang\", \"sdc\"]].values\r\n",
        "\r\n",
        "print(\"dijkstra_test_Xdata: \", dijkstra_test_Xdata.shape)\r\n",
        "\r\n",
        "x_test = dijkstra_test_Xdata;\r\n",
        "y_test = dijkstra_test_Ydata;\r\n",
        "\r\n",
        "# Se realiza un escalado de los datos\r\n",
        "y_test = y_test/100 # datos en tanto por 1 [0,1)\r\n",
        "scaler = preprocessing.StandardScaler() #Values with mean=0 and standard deviation=1\r\n",
        "x_test = scaler.fit_transform(x_test)\r\n",
        "\r\n",
        "print(\"x_test: \", x_test.shape)\r\n",
        "print(\"y_test: \", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnnDcASALx2S"
      },
      "source": [
        "### ENTRENAMIENTO NORMAL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IjdcIBiTH3I"
      },
      "source": [
        "# Declarar el modelo a utilizar antes de correr esta celda\r\n",
        "model = nn_model2()\r\n",
        "model.fit(x=x_data, y=y_data, validation_data=(x_test, y_test), batch_size=16, epochs=400, verbose=2)\r\n",
        "pesos = 'red2_extremos.hdf5'\r\n",
        "model.save_weights(pesos)\r\n",
        "\r\n",
        "# Resultado de la red para ver que tal ha funcionado el entrenamiento\r\n",
        "scores = model.evaluate(x_test, y_test, verbose=0)\r\n",
        "print(\"Precision: \", scores[1]*100, \"%\")\r\n",
        "print(\"Error medio absoluto: \", scores[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ku5xoOyxTH3M"
      },
      "source": [
        "# Correr esta celda para guardar los pesos en la carpeta del Drive\n",
        "import shutil\n",
        "save_path = '/content/drive/Shareddrives/NN4FT_DB/Exp_Adrian/Redes Entrenadas'\n",
        " \n",
        "ruta = shutil.move(pesos, save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk13wlIrNjIQ"
      },
      "source": [
        "### ENTRENAMIENTO CUSTOM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bteSu3XwNwHS"
      },
      "source": [
        "# Declarar el modelo a utilizar antes de correr esta celda\n",
        "# Con los parametros optimizados\n",
        "\n",
        "model = nn_optModel_T2()                                          \n",
        "model.fit(x=x_data, y=y_data, validation_data=(x_test, y_test), batch_size=16, epochs= 300 , verbose=2)\n",
        "pesos = 'red2_extremos_custom.hdf5'\n",
        "model.save_weights(pesos)\n",
        " \n",
        "# Resultado de la red para ver que tal ha funcionado el entrenamiento\n",
        "scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Precision: \", scores[1]*100, \"%\")\n",
        "print(\"Error medio absoluto: \", scores[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyDMBYQpNwHT"
      },
      "source": [
        "# Correr esta celda para guardar los pesos en la carpeta del Drive\n",
        "import shutil\n",
        "save_path = '/content/drive/Shareddrives/NN4FT_DB/Exp_Adrian/Redes Entrenadas'\n",
        " \n",
        "ruta = shutil.move(pesos, save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LK_EPmEBLx2S"
      },
      "source": [
        "### EVALUACIÓN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEBhPCXtTc36"
      },
      "source": [
        "# Comentar y descomentar segun que quieras probar\r\n",
        "# nn_path = '/content/drive/Shareddrives/NN4FT_DB/Exp_Adrian/Redes Entrenadas/red2_extremos.hdf5'\r\n",
        "nn_path = '/content/drive/Shareddrives/NN4FT_DB/Exp_Adrian/Redes Entrenadas/TAREA_2/RED2_EXTREMOS/red2_extremos_custom.hdf5'\r\n",
        "\r\n",
        "# Declarar el modelo a utilizar antes de correr esta celda\r\n",
        "model = nn_optModel_2() #Cambiar el modelo según que quieras probar (nn_model, nn_optModel_2)\r\n",
        "model.load_weights(nn_path) \r\n",
        "\r\n",
        "# Correr la red nueronal\r\n",
        "print('Corriendo la red ... ')\r\n",
        "scores = model.evaluate(x_test, y_test, verbose=0)\r\n",
        "print(\"Precision: \", scores[1]*100, \"%\")\r\n",
        "print(\"Error medio absoluto: \", scores[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFWAm49CTc4K"
      },
      "source": [
        "# Resultados [SDC, HANG] que obtiene la red para las entradas dadas\r\n",
        "y_predict = model.predict(x_test)\r\n",
        "\r\n",
        "# Error absoluto de las predicciones \r\n",
        "errores = np.abs(y_test-model.predict(x_test))\r\n",
        "\r\n",
        "# Error medio absoluto obtenido en las predicciones \r\n",
        "mean_error = np.abs(y_test-model.predict(x_test)).mean(axis=0)\r\n",
        "\r\n",
        "# Caulcular los resultados para UnAce\r\n",
        "unAce_predict = 1 - (y_predict[:,0] + y_predict[:,1])\r\n",
        "unAce_real = 1 - (y_test[:,0] + y_test[:,1])\r\n",
        "\r\n",
        "unAce_error = np.abs(unAce_real - unAce_predict)\r\n",
        "unAce_MAE = np.abs(unAce_real - unAce_predict).mean(axis=0)\r\n",
        "\r\n",
        "print('Error medio absoluto (FT) :')\r\n",
        "print('SDC -->', mean_error[0])\r\n",
        "print('HANG -->', mean_error[1])\r\n",
        "print('UnAce -->', unAce_MAE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTcxhu_BTc4M"
      },
      "source": [
        "# Dibuja todas las graficas de los datos de HANG y SDC\r\n",
        "dibujar_Graficas(y_predict, y_test, errores)\r\n",
        "# Dibuja todas las graficas de los datos de unAce\r\n",
        "graficas_UnAce(y_predict, y_test, errores, unAce_real, unAce_predict, unAce_error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfHvb7iJQ7_P"
      },
      "source": [
        "### GUARDADO DE LOS RESULTADOS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvfrSB9lTjH5"
      },
      "source": [
        "results_path = '/content/drive/Shareddrives/NN4FT_DB/Exp_Adrian/Resultados/'\r\n",
        "\r\n",
        "csv = 'red2_extremos_custom.csv' # ENTRADA DEL USUARIO\r\n",
        "with open(results_path + csv, \"w\") as f:\r\n",
        "  f.write('SDC_Real;HANG_Real;UnAce_Real;SDC_Predicho;HANG_Predicho;UnAce_Predicho;error_SDC;error_HANG;error_UnAce\\n')\r\n",
        "  f.close()\r\n",
        "\r\n",
        "string = \"\"\r\n",
        "\r\n",
        "for i in range(y_test.shape[0]):\r\n",
        "  string += (str(y_test[i,0]) + \";\" + str(y_test[i,1]) + \";\" + str(unAce_real[i]) + \";\"\r\n",
        "            + str(y_predict[i,0]) + \";\" + str(y_predict[i,1]) + \";\" + str(unAce_predict[i]) + \";\"\r\n",
        "            + str(errores[i,0]) + \";\" + str(errores[i,1]) + \";\" + str(unAce_error[i]) + \"\\n\")\r\n",
        "  \r\n",
        "  \r\n",
        "with open(results_path + csv, \"a\") as f:\r\n",
        "  f.write(string)\r\n",
        "  f.close()\r\n",
        "  "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3PAfW_gK_gn"
      },
      "source": [
        "## RED1 ORIGINAL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QPE1oR4LkCW"
      },
      "source": [
        "### DATOS ENTRENAMIENTO\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Bj-aQSST044"
      },
      "source": [
        "# Obtención de los datasets de entrenamiento \r\n",
        "bubblesort_path = '/content/drive/Shareddrives/NN4FT_DB/Exp_Adrian/Data Frames/TAREA_2/bubblesort_full.csv'\r\n",
        "dijkstra_path = '/content/drive/Shareddrives/NN4FT_DB/Exp_Adrian/Data Frames/TAREA_2/dijkstra_full.csv'\r\n",
        "ndes_path = '/content/drive/Shareddrives/NN4FT_DB/Exp_Adrian/Data Frames/TAREA_2/ndes_full.csv'\r\n",
        "\r\n",
        "bubblesort_df = pd.read_csv(bubblesort_path, sep=\";\")\r\n",
        "print(\"bubblesort_df: \", bubblesort_df.shape)\r\n",
        "\r\n",
        "dijkstra_df = pd.read_csv(dijkstra_path, sep=\";\")\r\n",
        "print(\"dijkstra_df: \", dijkstra_df.shape)\r\n",
        "\r\n",
        "ndes_df = pd.read_csv(ndes_path, sep=\";\")\r\n",
        "print(\"ndes_df: \", ndes_df.shape)\r\n",
        "\r\n",
        "# Obtención de los datos de entrenamiento\r\n",
        "\r\n",
        "bubblesort_Xdata = bubblesort_df[[\"time\", \"size\"]].values\r\n",
        "bubblesort_Ydata = bubblesort_df[[\"hang\", \"sdc\"]].values\r\n",
        "\r\n",
        "print(\"bubblesort_Xdata: \", bubblesort_Xdata.shape)\r\n",
        "\r\n",
        "dijkstra_Xdata = dijkstra_df[[\"time\", \"size\"]].values\r\n",
        "dijkstra_Ydata = dijkstra_df[[\"hang\", \"sdc\"]].values\r\n",
        "\r\n",
        "print(\"dijkstra_Xdata: \", dijkstra_Xdata.shape)\r\n",
        "\r\n",
        "ndes_Xdata = ndes_df[[\"time\", \"size\"]].values\r\n",
        "ndes_Ydata = ndes_df[[\"hang\", \"sdc\"]].values\r\n",
        "\r\n",
        "print(\"ndes_Xdata: \", ndes_Xdata.shape)\r\n",
        "\r\n",
        "x_data = np.concatenate((bubblesort_Xdata, dijkstra_Xdata, ndes_Xdata), axis=0)\r\n",
        "y_data = np.concatenate((bubblesort_Ydata, dijkstra_Ydata, ndes_Ydata), axis=0)\r\n",
        "\r\n",
        "# Escalado de los datos utilizados \r\n",
        "y_data = y_data/100 # Se deja en tanto por 1 [0,1)\r\n",
        "scaler = preprocessing.StandardScaler() #Values with mean=0 and standard deviation=1\r\n",
        "x_data = scaler.fit_transform(x_data) \r\n",
        "\r\n",
        "print(\"x_data: \", x_data.shape)\r\n",
        "print(\"y_data: \", y_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P9sDW_fLkCW"
      },
      "source": [
        "### DATOS DE TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ck-qQ1OxUI84"
      },
      "source": [
        "# Se separan los datasets de entrenamiento completos en 90% training y 10% test\r\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.1, random_state=0, shuffle=True)\r\n",
        "\r\n"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYQqmjrZLyQe"
      },
      "source": [
        "### ENTRENAMIENTO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVEc8v2dUn4M"
      },
      "source": [
        "# Declarar el modelo a utilizar antes de correr esta celda\r\n",
        "model = nn_model2()\r\n",
        "model.fit(x=x_train, y=y_train, validation_data=(x_test, y_test), batch_size=16, epochs=400, verbose=2)\r\n",
        "pesos = 'red1_original.hdf5'\r\n",
        "model.save_weights(pesos)\r\n",
        "\r\n",
        "# Resultado de la red para ver que tal ha funcionado el entrenamiento\r\n",
        "scores = model.evaluate(x_test, y_test, verbose=0)\r\n",
        "print(\"Precision: \", scores[1]*100, \"%\")\r\n",
        "print(\"Error medio absoluto: \", scores[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEGjPU-DUn4Q"
      },
      "source": [
        "# Correr esta celda para guardar los pesos en la carpeta del Drive\r\n",
        "import shutil\r\n",
        "save_path = '/content/drive/Shareddrives/NN4FT_DB/Exp_Adrian/Redes Entrenadas'\r\n",
        "\r\n",
        "ruta = shutil.move(pesos, save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsGVDEkwLyQf"
      },
      "source": [
        "### EVALUACIÓN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j82o-sZaUxst"
      },
      "source": [
        "nn_path = '/content/drive/Shareddrives/NN4FT_DB/Exp_Adrian/Redes Entrenadas/red1_original.hdf5'\r\n",
        "\r\n",
        "# Declarar el modelo a utilizar antes de correr esta celda\r\n",
        "model = nn_model()\r\n",
        "model.load_weights(nn_path)\r\n",
        "\r\n",
        "# Correr la red nueronal\r\n",
        "print('Corriendo la red ... ')\r\n",
        "scores = model.evaluate(x_test, y_test, verbose=0)\r\n",
        "print(\"Precision: \", scores[1]*100, \"%\")\r\n",
        "print(\"Error medio absoluto: \", scores[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deZDkq_xUxsy"
      },
      "source": [
        "# Resultados [SDC, HANG] que obtiene la red para las entradas dadas\r\n",
        "y_predict = model.predict(x_test)\r\n",
        "\r\n",
        "# Error absoluto de las predicciones \r\n",
        "errores = np.abs(y_test-model.predict(x_test))\r\n",
        "\r\n",
        "# Error medio absoluto obtenido en las predicciones \r\n",
        "mean_error = np.abs(y_test-model.predict(x_test)).mean(axis=0)\r\n",
        "\r\n",
        "# Caulcular los resultados para UnAce\r\n",
        "unAce_predict = 1 - (y_predict[:,0] + y_predict[:,1])\r\n",
        "unAce_real = 1 - (y_test[:,0] + y_test[:,1])\r\n",
        "\r\n",
        "unAce_error = np.abs(unAce_real - unAce_predict)\r\n",
        "unAce_MAE = np.abs(unAce_real - unAce_predict).mean(axis=0)\r\n",
        "\r\n",
        "print('Error medio absoluto (FT) :')\r\n",
        "print('SDC -->', mean_error[0])\r\n",
        "print('HANG -->', mean_error[1])\r\n",
        "print('UnAce -->', unAce_MAE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJL9gR-1Uxsy"
      },
      "source": [
        "# Dibuja todas las graficas de los datos de HANG y SDC\r\n",
        "dibujar_Graficas(y_predict, y_test, errores)\r\n",
        "# Dibuja todas las graficas de los datos de unAce\r\n",
        "graficas_UnAce(y_predict, y_test, errores, unAce_real, unAce_predict, unAce_error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZv_NiHsQ-TX"
      },
      "source": [
        "### GUARDADO DE LOS RESULTADOS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPNwmlbYVEV8"
      },
      "source": [
        "results_path = '/content/drive/Shareddrives/NN4FT_DB/Exp_Adrian/Resultados/'\r\n",
        "\r\n",
        "csv = 'red1_original.csv' # ENTRADA DEL USUARIO\r\n",
        "\r\n",
        "with open(results_path + csv, \"w\") as f:\r\n",
        "  f.write('SDC_Real;HANG_Real;UnAce_Real;SDC_Predicho;HANG_Predicho;UnAce_Predicho;error_SDC;error_HANG;error_UnAce\\n')\r\n",
        "  f.close()\r\n",
        "\r\n",
        "string = \"\"\r\n",
        "\r\n",
        "for i in range(y_test.shape[0]):\r\n",
        "  string += (str(y_test[i,0]) + \";\" + str(y_test[i,1]) + \";\" + str(unAce_real[i]) + \";\"\r\n",
        "            + str(y_predict[i,0]) + \";\" + str(y_predict[i,1]) + \";\" + str(unAce_predict[i]) + \";\"\r\n",
        "            + str(errores[i,0]) + \";\" + str(errores[i,1]) + \";\" + str(unAce_error[i]) + \"\\n\")\r\n",
        "  \r\n",
        "  \r\n",
        "with open(results_path + csv, \"a\") as f:\r\n",
        "  f.write(string)\r\n",
        "  f.close()\r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfVcMtpILJ6I"
      },
      "source": [
        "## RED2 ORIGINAL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txwc1EDbLkTt"
      },
      "source": [
        "### DATOS ENTRENAMIENTO\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMTD20PLVVGn"
      },
      "source": [
        "# Obtención de los datasets de entrenamiento \r\n",
        "bubblesort_path = '/content/drive/Shareddrives/NN4FT_DB/Exp_Adrian/Data Frames/TAREA_2/bubblesort_full.csv'\r\n",
        "ndes_path = '/content/drive/Shareddrives/NN4FT_DB/Exp_Adrian/Data Frames/TAREA_2/ndes_full.csv'\r\n",
        "\r\n",
        "bubblesort_df = pd.read_csv(bubblesort_path, sep=\";\")\r\n",
        "print(\"bubblesort_df: \", bubblesort_df.shape)\r\n",
        "\r\n",
        "ndes_df = pd.read_csv(ndes_path, sep=\";\")\r\n",
        "print(\"ndes_df: \", ndes_df.shape)\r\n",
        "\r\n",
        "full_df = pd.concat([bubblesort_df, ndes_df])\r\n",
        "print(\"full_df: \", full_df.shape)\r\n",
        "\r\n",
        "# Para obtener el dataset de test se debe indicar un porcentaje del tamaño total (~ 1542)\r\n",
        "porcentaje_ind = 75 # Entrada del usuario (Porcentaje)\r\n",
        "num_ind = (len(full_df)*porcentaje_ind)//100\r\n",
        "print(\"El numero de individuos para la realización de test son: \", num_ind)\r\n",
        "\r\n",
        "final_df = full_df.sample(num_ind).reset_index(drop=True)\r\n",
        "\r\n",
        "# Obtención de los datos de entrenamiento\r\n",
        "\r\n",
        "final_Xdata = final_df[[\"time\", \"size\"]].values\r\n",
        "final_Ydata = final_df[[\"hang\", \"sdc\"]].values\r\n",
        "\r\n",
        "print(\"final_Xdata: \", bubblesort_Xdata.shape)\r\n",
        "\r\n",
        "x_data = final_Xdata\r\n",
        "y_data = final_Ydata\r\n",
        "\r\n",
        "# Escalado de los datos utilizados \r\n",
        "y_data = y_data/100 # Se deja en tanto por 1 [0,1)\r\n",
        "scaler = preprocessing.StandardScaler() #Values with mean=0 and standard deviation=1\r\n",
        "x_data = scaler.fit_transform(x_data) \r\n",
        "\r\n",
        "print(\"x_data: \", x_data.shape)\r\n",
        "print(\"y_data: \", y_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnoKe2jrLkTt"
      },
      "source": [
        "### DATOS DE TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1RE7iAmW4_L"
      },
      "source": [
        "# Obtención de los datasets de test\r\n",
        "dijkstra_path = '/content/drive/Shareddrives/NN4FT_DB/Exp_Adrian/Data Frames/TAREA_2/dijkstra_full.csv'\r\n",
        "\r\n",
        "dijkstra_df = pd.read_csv(dijkstra_path, sep=\";\")\r\n",
        "print(\"dijkstra_df: \", dijkstra_df.shape)\r\n",
        "\r\n",
        "# Para obtener el dataset de test se debe indicar un porcentaje del tamaño total (112)\r\n",
        "porcentaje_ind = 75 # Entrada del usuario (Porcentaje)\r\n",
        "num_ind = (len(dijkstra_df)*porcentaje_ind)//100\r\n",
        "print(\"El numero de individuos para la realización de test son: \", num_ind)\r\n",
        "\r\n",
        "dijkstra_test_df = dijkstra_df.sample(num_ind).reset_index(drop=True)\r\n",
        "\r\n",
        "print(\"Tamaño del dataset de Test: \", dijkstra_test_df.shape)\r\n",
        "\r\n",
        "# Obtención de los datos de test\r\n",
        "dijkstra_test_Xdata = dijkstra_test_df[[\"time\", \"size\"]].values\r\n",
        "dijkstra_test_Ydata = dijkstra_test_df[[\"hang\", \"sdc\"]].values\r\n",
        "\r\n",
        "print(\"dijkstra_test_Xdata: \", dijkstra_test_Xdata.shape)\r\n",
        "\r\n",
        "x_test = dijkstra_test_Xdata;\r\n",
        "y_test = dijkstra_test_Ydata;\r\n",
        "\r\n",
        "# Se realiza un escalado de los datos\r\n",
        "y_test = y_test/100 # datos en tanto por 1 [0,1)\r\n",
        "scaler = preprocessing.StandardScaler() #Values with mean=0 and standard deviation=1\r\n",
        "x_test = scaler.fit_transform(x_test)\r\n",
        "\r\n",
        "print(\"x_test: \", x_test.shape)\r\n",
        "print(\"y_test: \", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0H33hpPrLyqX"
      },
      "source": [
        "### ENTRENAMIENTO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKIbm9slXREH"
      },
      "source": [
        "# Declarar el modelo a utilizar antes de correr esta celda\r\n",
        "model = nn_model2()\r\n",
        "model.fit(x=x_data, y=y_data, validation_data=(x_test, y_test), batch_size=16, epochs=400, verbose=2)\r\n",
        "pesos = 'red2_original.hdf5'\r\n",
        "model.save_weights(pesos)\r\n",
        "\r\n",
        "# Resultado de la red para ver que tal ha funcionado el entrenamiento\r\n",
        "scores = model.evaluate(x_test, y_test, verbose=0)\r\n",
        "print(\"Precision: \", scores[1]*100, \"%\")\r\n",
        "print(\"Error medio absoluto: \", scores[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdbzSmwQXREJ"
      },
      "source": [
        "# Correr esta celda para guardar los pesos en la carpeta del Drive\r\n",
        "import shutil\r\n",
        "save_path = '/content/drive/Shareddrives/NN4FT_DB/Exp_Adrian/Redes Entrenadas'\r\n",
        "\r\n",
        "ruta = shutil.move(pesos, save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMsmpaxJLyqX"
      },
      "source": [
        "### EVALUACIÓN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ls5TsuqfXYTk"
      },
      "source": [
        "nn_path = '/content/drive/Shareddrives/NN4FT_DB/Exp_Adrian/Redes Entrenadas/TAREA_2/RED2_ORIGINAL/red2_original.hdf5'\r\n",
        "\r\n",
        "# Declarar el modelo a utilizar antes de correr esta celda\r\n",
        "model = nn_model()\r\n",
        "model.load_weights(nn_path)\r\n",
        "\r\n",
        "# Correr la red nueronal\r\n",
        "print('Corriendo la red ... ')\r\n",
        "scores = model.evaluate(x_test, y_test, verbose=0)\r\n",
        "print(\"Precision: \", scores[1]*100, \"%\")\r\n",
        "print(\"Error medio absoluto: \", scores[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAzD5uKjXYTl"
      },
      "source": [
        "# Resultados [SDC, HANG] que obtiene la red para las entradas dadas\r\n",
        "y_predict = model.predict(x_test)\r\n",
        "\r\n",
        "# Error absoluto de las predicciones \r\n",
        "errores = np.abs(y_test-model.predict(x_test))\r\n",
        "\r\n",
        "# Error medio absoluto obtenido en las predicciones \r\n",
        "mean_error = np.abs(y_test-model.predict(x_test)).mean(axis=0)\r\n",
        "\r\n",
        "# Caulcular los resultados para UnAce\r\n",
        "unAce_predict = 1 - (y_predict[:,0] + y_predict[:,1])\r\n",
        "unAce_real = 1 - (y_test[:,0] + y_test[:,1])\r\n",
        "\r\n",
        "unAce_error = np.abs(unAce_real - unAce_predict)\r\n",
        "unAce_MAE = np.abs(unAce_real - unAce_predict).mean(axis=0)\r\n",
        "\r\n",
        "print('Error medio absoluto (FT) :')\r\n",
        "print('SDC -->', mean_error[0])\r\n",
        "print('HANG -->', mean_error[1])\r\n",
        "print('UnAce -->', unAce_MAE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdWw56l9XYTl"
      },
      "source": [
        "# Dibuja todas las graficas de los datos de HANG y SDC\r\n",
        "dibujar_Graficas(y_predict, y_test, errores)\r\n",
        "# Dibuja todas las graficas de los datos de unAce\r\n",
        "graficas_UnAce(y_predict, y_test, errores, unAce_real, unAce_predict, unAce_error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTwTeQlQQ9V3"
      },
      "source": [
        "### GUARDADO DE LOS RESULTADOS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtzbiuLLXaqJ"
      },
      "source": [
        "results_path = '/content/drive/Shareddrives/NN4FT_DB/Exp_Adrian/Resultados/'\r\n",
        "\r\n",
        "csv = 'red2_original.csv' # ENTRADA DEL USUARIO\r\n",
        "with open(results_path + csv, \"w\") as f:\r\n",
        "  f.write('SDC_Real;HANG_Real;UnAce_Real;SDC_Predicho;HANG_Predicho;UnAce_Predicho;error_SDC;error_HANG;error_UnAce\\n')\r\n",
        "  f.close()\r\n",
        "\r\n",
        "string = \"\"\r\n",
        "\r\n",
        "for i in range(y_test.shape[0]):\r\n",
        "  string += (str(y_test[i,0]) + \";\" + str(y_test[i,1]) + \";\" + str(unAce_real[i]) + \";\"\r\n",
        "            + str(y_predict[i,0]) + \";\" + str(y_predict[i,1]) + \";\" + str(unAce_predict[i]) + \";\"\r\n",
        "            + str(errores[i,0]) + \";\" + str(errores[i,1]) + \";\" + str(unAce_error[i]) + \"\\n\")\r\n",
        "  \r\n",
        "  \r\n",
        "with open(results_path + csv, \"a\") as f:\r\n",
        "  f.write(string)\r\n",
        "  f.close()\r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hgkxa3MpMhu6"
      },
      "source": [
        "## OPTMIZACIÓN DE HIPERPARAMETROS\r\n",
        "\r\n",
        "Para realizar la optimización de parametros en este proyecto solamente se realizará solamente para probar las `red1 extremos` y `red2 extremos`. Por esta razón antes de llevar a cabo la optimización de parametros se deben correr los apartados de `DATOS ENTRENAMIENTO` y `DATOS TEST` para los conjuntos de datos que se quiera hacer la optimización."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THyv7279f0Hv"
      },
      "source": [
        "# Hyperparameter optimization\n",
        "# Parametros a optimizar son : nl, nnl, epochs\n",
        " \n",
        "# Fix random seed for reproducibility\n",
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Antes de correr esta celda se debe declarar el modelo a optimizar\n",
        "model = KerasRegressor(build_fn=nn_modelToOptimize, epochs=200, verbose=0)\n",
        " \n",
        "# Neurons in each layer\n",
        "nn1=[2, 5, 15, 25, 45, 75, 128] \n",
        "nn2=[2, 5, 15, 25, 45, 75, 128]\n",
        "nn3=[2, 5, 15, 25, 45, 75, 128]\n",
        "nn4=[2, 5, 15, 25, 45, 75, 128]\n",
        "nn5=[2, 5, 15, 25, 45, 75, 128]\n",
        "nn6=[2, 5, 15, 25, 45, 75, 128]\n",
        "nn7=[2, 5, 15, 25, 45, 75, 128]\n",
        "\n",
        "# Numbers of hidden layers\n",
        "# nl = [1]\n",
        "# param_grid = dict(nl=nl, nn1=nn1) #nl = 1\n",
        "\n",
        "# nl = [2]\n",
        "# param_grid = dict(nl=nl, nn1=nn1, nn2=nn2) #nl = 2\n",
        "\n",
        "# nl = [3]\n",
        "# param_grid = dict(nl=nl, nn1=nn1, nn2=nn2, nn3=nn3) #nl = 3\n",
        "\n",
        "# nl = [4]\n",
        "# param_grid = dict(nl=nl, nn1=nn1, nn2=nn2, nn3=nn3, nn4=nn4) #nl = 4\n",
        "\n",
        "# nl = [5]\n",
        "# param_grid = dict(nl=nl, nn1=nn1, nn2=nn2, nn3=nn3, nn4=nn4, nn5=nn5) #nl = 5\n",
        "\n",
        "# nl = [6]\n",
        "# param_grid = dict(nl=nl, nn1=nn1, nn2=nn2, nn3=nn3, nn4=nn4, nn5=nn5, nn6=nn6) #nl = 6\n",
        "\n",
        "nl = [7]\n",
        "param_grid = dict(nl=nl, nn1=nn1, nn2=nn2, nn3=nn3, nn4=nn4, nn5=nn5, nn6=nn6, nn7=nn7) #nl = 7\n",
        " \n",
        "#grid = GridSearchCV(estimator=model, scoring='neg_mean_absolute_error', param_grid=param_grid, cv=10, n_jobs=-1, verbose=2)\n",
        "grid = RandomizedSearchCV(estimator=model, scoring='neg_mean_absolute_error', n_iter=36, param_distributions=param_grid, cv=10, n_jobs=-1, verbose=2)\n",
        "grid_result = grid.fit(x_data, y_data)\n",
        " \n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        " \n",
        "for mean, stdev, param in sorted(zip(means, stds, params), key=lambda x: x[0])[::-1]:\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4yX6Mhr48SA"
      },
      "source": [
        "# Función necesaria para hacer la evaluación de la Validación Cruzada\n",
        "\n",
        "def train_and_evaluate(model,train,test):\n",
        "\n",
        "  scaler = preprocessing.StandardScaler() #Values with mean=0 and standard deviation=1\n",
        "  \n",
        "  x_trainCV = scaler.fit_transform(x_data[train])\n",
        "  x_testCV = scaler.transform(x_data[test])\n",
        "  \n",
        "  y_trainCV = y_data[train]\n",
        "  y_testCV = y_data[test]\n",
        "  \n",
        "  history = model.fit(x=x_trainCV, y=y_trainCV, epochs=epochs, batch_size=16, validation_data=(x_testCV, y_testCV), verbose=0)\n",
        "  \n",
        "  scores = model.evaluate(x_testCV, y_testCV, verbose=0)\n",
        "  \n",
        "  pinta_grafica(history)\n",
        "  \n",
        "  return scores, history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pew3yCc-X3yI"
      },
      "source": [
        "def pinta_grafica(history): \n",
        "  \n",
        "  plt.plot(history.history['loss'],'r')\n",
        "  if('val_loss' in history.history.keys()):\n",
        "    plt.plot(history.history['val_loss'], 'b')\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6rS7IbAp1AV"
      },
      "source": [
        "# Para obtener un numero de épocas como \n",
        "epochs = 300 # Se utiliza un numero alto para ver donde deja de aprender\n",
        "n_folds = 10 # CV con 10 particiones\n",
        "\n",
        "skf = KFold(n_splits=n_folds, random_state=0, shuffle=True).split(x_data, y_data)\n",
        " \n",
        "mean_plot = dict(loss=[0 for i in range(epochs)],val_loss=[0 for i in range(epochs)])\n",
        "values_kfold = np.empty([n_folds, 2])\n",
        "\n",
        "for i, (train, test) in enumerate(skf):\n",
        "  print(\"Running fold \", i+1, \"/\", n_folds)\n",
        "\n",
        "  # Cambiar el modelo que se quiera\n",
        "  model = nn_optModel_T2() \n",
        "  c, history = train_and_evaluate(model,train,test)\n",
        "  values_kfold[i]=c\n",
        "  \n",
        "  for i in range(epochs):\n",
        "    mean_plot['loss'][i]+= history.history['loss'][i]\n",
        "    mean_plot['val_loss'][i]+= history.history['val_loss'][i]\n",
        " \n",
        "meanValues = values_kfold.mean(axis=0)\n",
        "print(\"Loss: \", meanValues[0], \" Accuracy: \", meanValues[1])\n",
        "\n",
        "for i in range(epochs):\n",
        "  mean_plot['loss'][i] /= n_folds\n",
        "  mean_plot['val_loss'][i] /= n_folds\n",
        "\n",
        "plt.plot(mean_plot['loss'],'r')\n",
        "plt.plot(mean_plot['val_loss'], 'b')\n",
        "plt.title('CV mean loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Qtk8ticL3QR"
      },
      "source": [
        "## MODELOS DE REDES NUERONALES\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jDNU1RnL7In"
      },
      "source": [
        "### MODELO 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JJudXoq5R9C"
      },
      "source": [
        "def nn_model():\r\n",
        "  visible = Input(shape=(x_data.shape[1],))\r\n",
        "  \r\n",
        "  hidden = Dense(128, activation='relu', kernel_initializer='he_normal') (visible)\r\n",
        "  hidden = Dense(128, activation='relu', kernel_initializer='he_normal') (hidden)\r\n",
        "  hidden = Dense(22, activation='relu', kernel_initializer='he_normal') (hidden)\r\n",
        "  hidden = Dense(22, activation='relu', kernel_initializer='he_normal') (hidden)\r\n",
        "  hidden = Dense(64, activation='relu', kernel_initializer='he_normal') (hidden)\r\n",
        "  hidden = Dense(44, activation='relu', kernel_initializer='he_normal') (hidden)\r\n",
        "  \r\n",
        "  output = Dense(2, activation='tanh', kernel_initializer='he_normal') (hidden)\r\n",
        "  \r\n",
        "  model = Model(visible, output)\r\n",
        "  model.compile(optimizer='Adamax', loss='mean_absolute_error', metrics=['accuracy'])\r\n",
        "  \r\n",
        "  return model"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnmMZoQmL-nz"
      },
      "source": [
        "### MODELO 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJeoJFCuvbdi"
      },
      "source": [
        "#Define your model\r\n",
        "def nn_model2():\r\n",
        "  visible = Input(shape=(x_data.shape[1]))\r\n",
        "  \r\n",
        "  hidden = Dense(128, activation='relu', kernel_initializer='he_normal', kernel_constraint=maxnorm(3)) (visible)\r\n",
        "  hidden = Dropout(0.1)(hidden)\r\n",
        "  hidden = Dense(128, activation='relu', kernel_initializer='he_normal', kernel_constraint=maxnorm(3)) (hidden)\r\n",
        "  hidden = Dropout(0.1)(hidden)\r\n",
        "  hidden = Dense(22, activation='relu', kernel_initializer='he_normal', kernel_constraint=maxnorm(3)) (hidden)\r\n",
        "  hidden = Dropout(0.1)(hidden)\r\n",
        "  hidden = Dense(22, activation='relu', kernel_initializer='he_normal', kernel_constraint=maxnorm(3)) (hidden)\r\n",
        "  hidden = Dropout(0.1)(hidden)\r\n",
        "  hidden = Dense(64, activation='relu', kernel_initializer='he_normal', kernel_constraint=maxnorm(3)) (hidden)\r\n",
        "  hidden = Dropout(0.1)(hidden)\r\n",
        "  hidden = Dense(44, activation='relu', kernel_initializer='he_normal', kernel_constraint=maxnorm(3)) (hidden)\r\n",
        "  hidden = Dropout(0.1)(hidden)\r\n",
        "  \r\n",
        "  output = Dense(2, activation='tanh', kernel_initializer='he_normal') (hidden)\r\n",
        "  \r\n",
        "  model = Model(visible, output)\r\n",
        "  model.compile(optimizer=optimizers.Adamax(), loss='mean_absolute_error', metrics=['accuracy'])\r\n",
        "  \r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZ9eC5UqL-ub"
      },
      "source": [
        "### MODELO PARA OPTIMIZAR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMlFO5ILi9ho"
      },
      "source": [
        "def nn_modelToOptimize(optimizer='Adamax', activation='relu', activation_output='tanh', \n",
        "                       loss='mean_absolute_error', init_mode='he_normal', \n",
        "                       nl=6, nn1=128, nn2=128, nn3=22, nn4=22, nn5=64, nn6=44, nn7=11):\n",
        "  \n",
        "  visible = Input(shape=(x_data.shape[1],))\n",
        "  \n",
        "  first = True\n",
        "  weights = [nn1, nn2, nn3, nn4, nn5, nn6, nn7]\n",
        "  for i in range(nl):\n",
        "    if first:\n",
        "      hidden = Dense(weights[i], activation=activation, kernel_initializer=init_mode) (visible)\n",
        "      first = False\n",
        "    else:\n",
        "      hidden = Dense(weights[i], activation=activation, kernel_initializer=init_mode) (hidden)\n",
        " \n",
        "  \n",
        "  output = Dense(2, activation=activation_output, kernel_initializer=init_mode) (hidden)\n",
        "  \n",
        "  model = Model(visible, output)\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "  \n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8Ukm8rM2ZSO"
      },
      "source": [
        "### MODELO OPTIMIZADO 1\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS0xEtrqOitl"
      },
      "source": [
        "#Define your model # Training model \r\n",
        "def nn_optModel_T1():\r\n",
        "  visible = Input(shape=(x_data.shape[1]))\r\n",
        "  \r\n",
        "  hidden = Dense(128, activation='relu', kernel_initializer='he_normal', kernel_constraint=maxnorm(3)) (visible)\r\n",
        "  hidden = Dropout(0.1)(hidden) # Solo para entrenamiento (evita overfitting)\r\n",
        "  hidden = Dense(15, activation='relu', kernel_initializer='he_normal', kernel_constraint=maxnorm(3)) (hidden)\r\n",
        "  hidden = Dropout(0.1)(hidden)\r\n",
        "  hidden = Dense(128, activation='relu', kernel_initializer='he_normal', kernel_constraint=maxnorm(3)) (hidden)\r\n",
        "  hidden = Dropout(0.1)(hidden)\r\n",
        "  hidden = Dense(128, activation='relu', kernel_initializer='he_normal', kernel_constraint=maxnorm(3)) (hidden)\r\n",
        "  hidden = Dropout(0.1)(hidden)\r\n",
        "  hidden = Dense(15, activation='relu', kernel_initializer='he_normal', kernel_constraint=maxnorm(3)) (hidden)\r\n",
        "  hidden = Dropout(0.1)(hidden)\r\n",
        "  \r\n",
        "  output = Dense(2, activation='tanh', kernel_initializer='he_normal') (hidden)\r\n",
        "  \r\n",
        "  model = Model(visible, output)\r\n",
        "  model.compile(optimizer=optimizers.Adamax(), loss='mean_absolute_error', metrics=['accuracy'])\r\n",
        "  \r\n",
        "  return model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuoyXqPNOzOE"
      },
      "source": [
        "def nn_optModel_1():\r\n",
        "  visible = Input(shape=(x_data.shape[1]))\r\n",
        "  \r\n",
        "  hidden = Dense(128, activation='relu', kernel_initializer='he_normal', kernel_constraint=maxnorm(3)) (visible)\r\n",
        "  hidden = Dense(15, activation='relu', kernel_initializer='he_normal', kernel_constraint=maxnorm(3)) (hidden)\r\n",
        "  hidden = Dense(128, activation='relu', kernel_initializer='he_normal', kernel_constraint=maxnorm(3)) (hidden)\r\n",
        "  hidden = Dense(128, activation='relu', kernel_initializer='he_normal', kernel_constraint=maxnorm(3)) (hidden)\r\n",
        "  hidden = Dense(15, activation='relu', kernel_initializer='he_normal', kernel_constraint=maxnorm(3)) (hidden)\r\n",
        "  \r\n",
        "  output = Dense(2, activation='tanh', kernel_initializer='he_normal') (hidden)\r\n",
        "  \r\n",
        "  model = Model(visible, output)\r\n",
        "  model.compile(optimizer=optimizers.Adamax(), loss='mean_absolute_error', metrics=['accuracy'])\r\n",
        "  \r\n",
        "  return model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6bFQRzeN6_4"
      },
      "source": [
        "### MODELO OPTIMIZADO 2\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3jtEqT4PLUd"
      },
      "source": [
        "#Define your model # Training model \r\n",
        "def nn_optModel_T2():\r\n",
        "  visible = Input(shape=(x_data.shape[1]))\r\n",
        "  \r\n",
        "  hidden = Dense(128, activation='relu', kernel_initializer='he_normal', kernel_constraint=maxnorm(3)) (visible)\r\n",
        "  hidden = Dropout(0.1)(hidden) # Solo para entrenamiento (evita overfitting)\r\n",
        "  hidden = Dense(128, activation='relu', kernel_initializer='he_normal', kernel_constraint=maxnorm(3)) (hidden)\r\n",
        "  hidden = Dropout(0.1)(hidden)\r\n",
        "  hidden = Dense(45, activation='relu', kernel_initializer='he_normal', kernel_constraint=maxnorm(3)) (hidden)\r\n",
        "  hidden = Dropout(0.1)(hidden)\r\n",
        "  hidden = Dense(75, activation='relu', kernel_initializer='he_normal', kernel_constraint=maxnorm(3)) (hidden)\r\n",
        "  hidden = Dropout(0.1)(hidden)\r\n",
        "  hidden = Dense(128, activation='relu', kernel_initializer='he_normal', kernel_constraint=maxnorm(3)) (hidden)\r\n",
        "  hidden = Dropout(0.1)(hidden)\r\n",
        "  hidden = Dense(25, activation='relu', kernel_initializer='he_normal', kernel_constraint=maxnorm(3)) (hidden)\r\n",
        "  hidden = Dropout(0.1)(hidden)\r\n",
        "  \r\n",
        "  output = Dense(2, activation='tanh', kernel_initializer='he_normal') (hidden)\r\n",
        "  \r\n",
        "  model = Model(visible, output)\r\n",
        "  model.compile(optimizer=optimizers.Adamax(), loss='mean_absolute_error', metrics=['accuracy'])\r\n",
        "  \r\n",
        "  return model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0FuFVywPLUt"
      },
      "source": [
        "def nn_optModel_2():\r\n",
        "  visible = Input(shape=(x_data.shape[1]))\r\n",
        "  \r\n",
        "  hidden = Dense(128, activation='relu', kernel_initializer='he_normal', kernel_constraint=maxnorm(3)) (visible)\r\n",
        "  hidden = Dense(128, activation='relu', kernel_initializer='he_normal', kernel_constraint=maxnorm(3)) (hidden)\r\n",
        "  hidden = Dense(45, activation='relu', kernel_initializer='he_normal', kernel_constraint=maxnorm(3)) (hidden)\r\n",
        "  hidden = Dense(75, activation='relu', kernel_initializer='he_normal', kernel_constraint=maxnorm(3)) (hidden)\r\n",
        "  hidden = Dense(128, activation='relu', kernel_initializer='he_normal', kernel_constraint=maxnorm(3)) (hidden)\r\n",
        "  hidden = Dense(25, activation='relu', kernel_initializer='he_normal', kernel_constraint=maxnorm(3)) (hidden)\r\n",
        "  \r\n",
        "  output = Dense(2, activation='tanh', kernel_initializer='he_normal') (hidden)\r\n",
        "  \r\n",
        "  model = Model(visible, output)\r\n",
        "  model.compile(optimizer=optimizers.Adamax(), loss='mean_absolute_error', metrics=['accuracy'])\r\n",
        "  \r\n",
        "  return model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYzV5uQNMBGJ"
      },
      "source": [
        "## FUNCIONES PARA GRAFICAR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCSGUnS8YREH"
      },
      "source": [
        "image_path = '/content/drive/Shareddrives/NN4FT_DB/Exp_Adrian/Graficas/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69EHkSig7FLb"
      },
      "source": [
        "def dibujar_Graficas(predicho, real, errores): \r\n",
        "  \r\n",
        "  figura_1 = plt.figure()\r\n",
        "  plt.title('SDC (Real vs Predicho)')\r\n",
        "  plt.plot(real[:,0],'r+')\r\n",
        "  plt.plot(predicho[:,0], 'b+')\r\n",
        "  plt.legend(['Real', 'Predicho'], loc='upper left')\r\n",
        "  plt.show()\r\n",
        "  figura_1.savefig(image_path + \"SDC_RP.png\")\r\n",
        "  plt.clf\r\n",
        "  print('\\n')\r\n",
        "  \r\n",
        "  figura_2 = plt.figure()\r\n",
        "  plt.title('HANG (Real vs Predicho)')\r\n",
        "  plt.plot(real[:,1],'r+')\r\n",
        "  plt.plot(predicho[:,1], 'b+')\r\n",
        "  plt.legend(['Real', 'Predicho'], loc='upper left')\r\n",
        "  plt.show()\r\n",
        "  figura_2.savefig(image_path + \"HANG_RP.png\")\r\n",
        "  plt.clf\r\n",
        "  print('\\n')\r\n",
        "\r\n",
        "  figura_3 = plt.figure()\r\n",
        "  plt.title('SDC y HANG (Real)')\r\n",
        "  plt.plot(real[:,0],'r+')\r\n",
        "  plt.plot(real[:,1], 'b+')\r\n",
        "  plt.legend(['SDC', 'HANG'], loc='upper left')\r\n",
        "  plt.show()\r\n",
        "  figura_3.savefig(image_path + \"SDCyHANG_R.png\")\r\n",
        "  plt.clf\r\n",
        "  print('\\n')\r\n",
        "\r\n",
        "  figura_4 = plt.figure()\r\n",
        "  plt.title('SDC y HANG (Predicho)')\r\n",
        "  plt.plot(predicho[:,0],'r+')\r\n",
        "  plt.plot(predicho[:,1], 'b+')\r\n",
        "  plt.legend(['SDC', 'HANG'], loc='upper left')\r\n",
        "  plt.show()\r\n",
        "  figura_4.savefig(image_path + \"SDCyHANG_P.png\")\r\n",
        "  plt.clf\r\n",
        "  print('\\n')\r\n",
        "\r\n",
        "  figura_5 = plt.figure()\r\n",
        "  plt.title('Errores absoultos')\r\n",
        "  plt.plot(errores[:,0],'r+')\r\n",
        "  plt.plot(errores[:,1], 'b+')\r\n",
        "  plt.legend(['SDC', 'HANG'], loc='upper left')\r\n",
        "  plt.show()\r\n",
        "  figura_5.savefig(image_path + \"Errores.png\")\r\n",
        "  plt.clf\r\n",
        "  print('\\n')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGucORqQkU_h"
      },
      "source": [
        "def graficas_UnAce(predicho, real, errores, unAce_r, unAce_p, unAce_e):\r\n",
        "\r\n",
        "  figura_6 = plt.figure()\r\n",
        "  plt.title('UnAce (Real vs Predicho)')\r\n",
        "  plt.plot(unAce_r,'r+')\r\n",
        "  plt.plot(unAce_p, 'b+')\r\n",
        "  plt.legend(['Real', 'Predicho'], loc='upper left')\r\n",
        "  plt.show()\r\n",
        "  figura_6.savefig(image_path + \"unAce_RP.png\")\r\n",
        "  plt.clf\r\n",
        "  print('\\n')\r\n",
        "\r\n",
        "  figura_7 = plt.figure()\r\n",
        "  plt.title('SDC HANG y UnAce (Real)')\r\n",
        "  plt.plot(real[:,0],'r+')\r\n",
        "  plt.plot(real[:,1], 'b+')\r\n",
        "  plt.plot(unAce_r, 'g+')\r\n",
        "  plt.legend(['SDC', 'HANG', 'UnAce'], loc='upper left')\r\n",
        "  plt.show()\r\n",
        "  figura_7.savefig(image_path + \"SDCyHANGyUnAce_R.png\")\r\n",
        "  plt.clf\r\n",
        "  print('\\n')\r\n",
        "\r\n",
        "  figura_8 = plt.figure()\r\n",
        "  plt.title('SDC HANG y UnAce (Predicho)')\r\n",
        "  plt.plot(predicho[:,0],'r+')\r\n",
        "  plt.plot(predicho[:,1], 'b+')\r\n",
        "  plt.plot(unAce_p, 'g+')\r\n",
        "  plt.legend(['SDC', 'HANG', 'UnAce'], loc='upper left')\r\n",
        "  plt.show()\r\n",
        "  figura_8.savefig(image_path + \"SDCyHANGyUnAce_P.png\")\r\n",
        "  plt.clf\r\n",
        "  print('\\n')\r\n",
        "\r\n",
        "  figura_9 = plt.figure()\r\n",
        "  plt.title('Errores absoultos')\r\n",
        "  plt.plot(errores[:,0],'r+')\r\n",
        "  plt.plot(errores[:,1], 'b+')\r\n",
        "  plt.plot(unAce_e, 'g+')\r\n",
        "  plt.legend(['SDC', 'HANG', 'UnAce'], loc='upper left')\r\n",
        "  plt.show()\r\n",
        "  figura_9.savefig(image_path + \"Errores_con_UnAce.png\")\r\n",
        "  plt.clf\r\n",
        "  print('\\n')\r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}